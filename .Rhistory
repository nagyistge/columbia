update(this_moment, hours = 10, minutes = 16, seconds = 0)
update(this_moment, hours = 10, minutes = 16, seconds = 0)
update(this_moment, hours = 10, minutes = 16, seconds = 0)
info()
update(this_moment, hours = 10, minutes = 16, seconds = 0)
skip()
this_moment
nyc <- now("America/New_York")
nyc
depart <- nyc + days(2)
depart
depart <- update(depart, hours = 17, minutes = 34)
depart
arrive <- nyc + hours(15) + minutes(50)
arrive <- depart + hours(15) + minutes(50)
?with_tz
arrive <- with_tz(arrive, tzone = "Asia/Hong_Kong")
arrive
last_time <- mdy("June 17, 2008", tz = "Singapore")
last_time
?new_interval
how_long <- new_interval(last_time, arrive, tzone = attr(last_time, "Singapore"))
how_long <- new_interval(last_time, arrive)
as.period(how_long)
stopwatch()
library(twitteR)
key <- "6iJMlFVaHjebp8MTnuhX7PSvZ"
secret <- "4beQ8LVZggujwxVHiIkyf4EWTZ1U55V14uDVzrCr90v1Z6bovM"
setup_twitter_oauth(key, secret)
token <- "3024233079-ok7pL5tkFoHNG80VY4aTDl3rMp87C0SL7ftv6M7"
token_secret <- "iEvWDVDEPWPKWeqbHv47cZvh1DdQ8kKb4UKA031nb2ndP"
setup_twitter_oauth(key, secret, access_token = token, access_secret = token_secret)
showStatus("123")
getUser("ShenJunfei")
getUser("EmWatson")
getUser("geoffjentry")
getUser("BarackObama")
userTimeline("EmWatson", n=100)
getUser("EmWatson")
getFollowers()
getFollowers(10)
?getFollowers
??getFollowers
favorites("EmWatson", n = 20)
ewf <- favorites("EmWatson", n = 20)
data.frame(ewf)
ewf[1]
dm <- dmFactory$new(text=􏰇foo􏰇, recipientSN=􏰇blah􏰇)
?getFollowers
?getFollowers("EmWatson")
getFollowers("EmWatson")
searchTwitter("@BarackObama", 10, lang="en")
em <- getUser("EmWatson")
em$getFollowers
em$getFollowers()
em$getFollowersCount()
em$getFriends(n=10)
em$getFriends(n=5)
em$getLongitude
em$getLongitude()
f <- em$getFriends()
em$getDescription
em$getDescription()
em$getFriends(n=1)
em$getFriends(n=5)
em$statusesCount()
em$statusesCount
class(em$statusesCount)
class(em$statusesCount[1])
em$friendsCount()
em$friendsCount
em$created
class(em$created)
em$favouriteCount
em$favoriteCount
em$favoriteCount()
em$getFollow()
em$getFollow
em$getFollowCount()
em$getFollowCount
names(em)
dim(em)
em[1]
em$description
em$name
js <- getUser("ShenJunfei")
js <- getUser("ShenJunfei")
em <- getUser("EmWatson")
x <- c(5.1, 4.9, 4.7, 4.6, 5.0)
y <- c(3.5, 3.0, 3.2, 3.1, 5.4)
cor(x, y)
x <- c(7.0, 6.4, 6.9, 5.5, 6.5)
y <- c(3.2, 3.2, 3.1, 2.3, 2.8)
cor(x,y)
install.packages("rCharts")
install.packages("rCharts")
install.packages("base64enc")
install.packages("rCharts")
require(devtools)
install_github('rCharts', 'ramnathv')
library(devtools)
install.packages("devtools")
require(devtools)
install_github('rCharts', 'ramnathv')
library(rCharts)
library(ggplot)
library(ggplots)
library(ggplot2)
install.pakcages("ggplot2")
install.packages("ggplot2")
install_github("ropensci/plotly")
library(plotly)
load(couseraData.rda)
load(courseraData.rda)
load("courseraData.rda")
load("courseraData.rda")
library(plotly)
library(shiny)
instaall.packages("shiny")
install.packages("shiny")
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text')
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
install.packages("yhatr")
library(yhatr)
install.packages("yhatr")
d1 <- c(1, 0.05, 0.9, .8)
d2 <- c(0.05, 1, 0.01, .5)
d3 <- c(0.9, 0.01, 1, 0.7)
d4 <- c(0.8, 0.5, .7, 1)
sqrt(sum(d4 - d1)^2)
sqrt(sum(d4 - d2)^2)
sqrt(sum(d4 - d3)^2)
library(twitteR)
key <- "6iJMlFVaHjebp8MTnuhX7PSvZ"
secret <- "4beQ8LVZggujwxVHiIkyf4EWTZ1U55V14uDVzrCr90v1Z6bovM"
token <- "3024233079-ok7pL5tkFoHNG80VY4aTDl3rMp87C0SL7ftv6M7"
token_secret <- "iEvWDVDEPWPKWeqbHv47cZvh1DdQ8kKb4UKA031nb2ndP"
myName <- "ShenJunfei"
setup_twitter_oauth(key, secret, access_token = token, access_secret = token_secret)
me <- getUser(myName)
install.packages("praise")
library(praise)
praise()
praise()
praise()
praise()
praise()
praise()
praise()
praise()
praise()
praise()
praise()
install.packages("Rweibo", repos = "http://R-Forge.R-project.org")
library(Rweibo)
?registerApp
registerApp(app_name=J.Shen, app_key=4268054691, app_secret=05543d9aed56db4e6857e618bee4ff86)
registerApp(J.Shen, 4268054691, 05543d9aed56db4e6857e618bee4ff86)
registerApp("J.Shen", "4268054691", "05543d9aed56db4e6857e618bee4ff86")
?Rweibo
??Rweibo
statuses.count()
roauth <- createOAuth("J.Shen", "Q_E_D")
roauth <- createOAuth("J.Shen", "Q_E_D")
roauth <- createOAuth("J.Shen", "Q_E_D", authorize=TRUE, login=FALSE, username="497739328@qq.com", password="371620sjf")
library(Rweibo)
registerApp("J.Shen", "4268054691", "05543d9aed56db4e6857e618bee4ff86")
library(Rweibo)
registerApp("J.Shen", "4268054691", "05543d9aed56db4e6857e618bee4ff86")
roauth <- createOAuth(app_name="J.Shen", access_name="rweibo")
require(Rweibo)
registerApp(app_name = "SNA3", "********", "****************")
roauth <- createOAuth(app_name = "SNA3", access_name = "rweibo")
require(Rwordseg)
install.packages(Rwordseg)
??Rwordseg
?Rwordseg
??Rwordseg
install.packages("Rwordseg", repos = "http://R-Forge.R-project.org")
roauth <- createOAuth(app_name="J.Shen", access_name="Q_E_D")
library(tm)
install.packages("tm")
install.packages("rJava")
??tmcn
?tmcn
install.packages("tmcn")
install.packages("tmcn", repos = "http://R-Forge.R-project.org")
install.packages("tmcn")
install.packages("~/Downloads/tmcn_0.1-3.tar", repos=NULL, type="source")
install.packages("~/Downloads/tmcn_0.1-4.tar", repos=NULL, type="source")
install.packages("~/Downloads/tmcn_0.1-4.tar", repos=NULL, type="source")
install.packages("~/Downloads/tmcn", repos=NULL, type="source")
library(tmcn)
library(tmcn)
?tmcn
??tmcn
Rweibo
install.packages("~/Downloads/Rweibo", repos=NULL, type="source")
library(Rweibo)
library(tmcn)
registerApp("J.Shen", "4268054691", "05543d9aed56db4e6857e618bee4ff86")
roauth <- createOAuth(app_name="J.Shen", access_name="Q_E_D")
install.packages("bitops")
install.packages("bitops")
install.packages("XML")
install.packages("Rjson")
install.packages("rjson")
install.packages("RCurl")
install.packages("digest")
roauth <- createOAuth(app_name="J.Shen", access_name="Q_E_D")
library(Rweibo)
roauth <- createOAuth(app_name="J.Shen", access_name="Q_E_D")
??registerApp
install.packages('stringi')
version()
getVersion()
verson
version
install.packages("quanteda")
require(quanteda)
summary(ie2010Corpus)
ieDfm <- dfm(ie2010Corpus, ignoredFeatures = c(stopwords("english"), "will"), stem = TRUE)
topfeatures(ieDfm)
plot(ieDfm, min.freq=25, random.order=FALSE)
warnings()
getwd()
help(package="quanteda")
summary(ukimmigTexts)
str(ukimmigTexts)
ukimmigTexts
encoding(ukimmigTexts)
encoding(encodedTexts)
endodedTexts
encodedTexts
immigCorpus <- corpus(ukimmigTexts, notes="Created as part of a demo.")
immigCorpus
immigCorpus[1]
immigCorpus[9]
immigCorpus[4]
immigCorpus[3]
immigCorpus[2]
docvars(immigCorpus) <- data.frame(party = docnames(immigCorpus), year = 2010)
summary(immigCorpus)
kwic(immigCorpus, "deport", window = 3)
kwic(immigCorpus, "illegal immig*", window = 3)
immigDfm <- dfm(subset(immigCorpus, party=="BNP"))
plot(immigDfm)
immigDfm <- dfm(subset(immigCorpus, party=="BNP"), ignoredFeatures = stopwords("english"))
plot(immigDfm, random.color = TRUE, rot.per = .25, colors = sample(colors()[2:128], 5))
immigCorpusSent <- changeunits(immigCorpus, to = "sentences")
immigCorpusSent
immigCorpusSent[1]
summary(immigCorpusSent, 20)
txt <- "#TextAnalysis is MY <3 4U @myhandle gr8 #stuff :-)"
tokenize(txt, removePunct=TRUE)
tokenize(txt, removePunct=TRUE, removeTwitter=TRUE)
(toks <- tokenize(toLower(txt), removePunct=TRUE, removeTwitter=TRUE))
str(toks)
toks
toks[1]
(sents <- tokenize(ukimmigTexts[1], what = "sentence", simplify = TRUE)[1:5])
sens
sents
tokenize(ukimmigTexts[1], what = "character", simplify = TRUE)[1:100]
summary(inaugCorpus)
presDfm <- dfm(inaugCorpus)
presDfm
docnames(presDfm)
presDfm <- dfm(inaugCorpus, groups="President")
presDfm
docnames(presDfm)
data(iebudgetsCorpus, package = "quantedaData")
summary(iebudgetsCorpus, 10)
data(iebudgetsCorpus, package = "quanteData")
library(quantedaData)
install.packages("quantedaData")
require(quanteda)
data(iebudgetsCorpus, package = "quantedaData")
data(iebudgetsCorpus, package = "quanteda")
??quantedaData
devtools::install_github("kbenoit/quantedaData")
data(iebudgetsCorpus, package = "quantedaData")
summary(iebudgetsCorpus, 10)
ieFinMin <- subset(iebudgetsCorpus, number=="01" & debate == "BUDGET")
ieFinMin
ieFinMin[1]
summary(ieFinMin)
dfmFM <- dfm(ieFinMin)
plot(2008:2012, lexdiv(dfmFM, "C"), xlab="Year", ylab="Herndan's C", type="b",
main = "World's Crudest Lexical Diversity Plot")
data(SOTUCorpus, package = "quantedaData")
fk <- readability(SOTUCorpus, "Flesch.Kincaid")
year <- lubridate::year(docvars(SOTUCorpus, "Date"))
library(lubridate)
install.packages("lubridate")
library(lubridate)
year <- lubridate::year(docvars(SOTUCorpus, "Date"))
year
require(ggplot2)
partyColours <- c("blue", "blue", "black", "black", "red", "red")
p <- ggplot(data = docvars(SOTUCorpus), aes(x = year, y = fk)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2, linetype=1, color="grey70", method = "loess", span = .34) +
xlab("") +
ylab("Flesch-Kincaid") +
geom_point(aes(colour = party)) +
scale_colour_manual(values = partyColours) +
geom_line(aes(), alpha=0.3, size = 1) +
ggtitle("Text Complexity in State of the Union Addresses") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
quartz(height=7, width=12)
print(p)
presDfm <- dfm(inaugCorpus, ignoredFeatures = stopwords("english"))
similarity(presDfm, "1985-Reagan", n=5, margin="documents")
similarity(presDfm, c("2009-Obama" , "2013-Obama"), n=5, margin="documents", method = "cosine")
similarity(presDfm, c("2009-Obama" , "2013-Obama"), n=5, margin="documents", method = "Hellinger")
similarity(presDfm, c("2009-Obama" , "2013-Obama"), n=5, margin="documents", method = "eJaccard")
similarity(presDfm, c("fair", "health", "terror"), method="cosine")
txt <- "Hey @kenbenoit #textasdata: The quick, brown fox jumped over the lazy dog!"
(toks1 <- tokenize(toLower(txt), removePunct = TRUE))
tokenize(toLower(txt), removePunct = TRUE, ngrams = 2)
tokenize(toLower(txt), removePunct = TRUE, ngrams = c(1,3))
ngrams(tokens, c(1, 3, 5))
tokens <- tokenize(toLower("Insurgents killed in ongoing fighting."),
removePunct = TRUE, simplify = TRUE)
ngrams(tokens, c(1, 3, 5))
skipgrams(tokens, n = 2, k = 2, concatenator = " ")
skipgrams(tokens, n = 3, k = 2, concatenator = " ")
collocs2 <- collocations(inaugTexts, size = 2, method = "all")
head(collocs2, 20)
collocs3 <- collocations(inaugTexts, size = 3, method = "all")
head(collocs3, 20)
head(removeFeatures(collocs2, stopwords("english")), 20)
head(removeFeatures(collocs3, stopwords("english")), 20)
summary(inaugTexts)
summary(inaugTexts[1:5])
oneText <- inaugTexts[1]
oneText[2]
tmp <- inaugTexts[1:5]
length(inaugTexts)
length(oneText)
nchar(oneText)
nchar(inaugTexts[5:7])
?tokenize
tokens <- tokenize('Today is Thursday in Canberra. It is yesterday in London.')
tokens
vec <- c(one='This is text one', two='This, however, is the second text')
tokenize(vec)
tokenize(toLower(vec), removePunct = TRUE)
require(dplyr)
library(dplyr)
install.packages("dplyr")
library(dplyr)
require(dplyr)
inaugTokens <- tokenize(toLower(inaugTexts))
inaugTokens[2]
inaugDfm <- dfm(inaugTokens)
trimmedInaugDfm <- trim(inaugDfm, minDoc=5, minCount=10)
weightedTrimmedDfm <- weight(trimmedInaugDfm, type='tfidf')
require(dplyr)
inaugDfm2 <- dfm(inaugTokens) %>% trim(minDoc=5, minCount=10) %>% weight(type='tfidf')
methods(dfm)
methods(class = "tokenizedTexts")
summary(inaugTexts[52:57])
dv <- data.frame(Party = c('dem','dem','rep','rep','dem','dem'))
recentCorpus <- corpus(inaugTexts[52:57], docvars=dv)
summary(recentCorpus)
partyDfm <- dfm(recentCorpus, groups='Party', ignoredFeatures=(stopwords('english')))
wordcloud::comparison.cloud(t(as.matrix(partyDfm)))
require(quanteda)
s1 <- 'my example text'
length(s1)
nchar(s1)
s2 <- c('This is', 'my example text.', 'So imaginative.')
nchar(s2)
sum(nchar(s2))
which.max(nchar(inaugTexts))
inaguTexts
inaugTexts
which.min(nchar(inaugTexts))
s1 <- 'This file contains many fascinating example sentences.'
s1[6:9]
s1 <- 'This file contains many fascinating example sentences.'
substr(s1, 6,9)
names(inaugTexts)
s1 <- 'split this string'
strsplit(s1, 'this')
parts <- strsplit(names(inaugTexts), '-')
years <- sapply(parts, function(x) x[1])
parts
years
pres <-  sapply(parts, function(x) x[2])
paste('one','two','three')
paste('one','two','three', sep='_')
paste(years, pres, sep='-')
paste(years, pres, collapse='-')
tolower(s1)
toupper(s1)
tolower(c("This", "is", "Kεφαλαία Γράμματα"))
methods(toLower)
tolower(s1) == toupper(s1)
'apples'=='oranges'
c1 <- c('apples', 'oranges', 'pears')
'pears' %in% c1
grep('orangef', 'these are oranges')
require(quanteda)
summary(inaugTexts)
summary(inaugTexts[1:5])
inaugTexts
class(inaugTexts)
inaugTexts[1]
length(inaugTexts)
library(ggmap)
library(ggplot2)
install.packages(ggmap)
install.packages("ggmap")
library(ggmap)
library(ggplot2)
nycmap <- get_map("New York City", zoom=11)
nyc <- ggmap(nycmap) + xlim (-74.1, -73.8) + ylim (40.7, 40.85)
long <- c(-73.99313973, -73.86915874, -73.98569489, -73.96860943, -73.96086502, -73.95382469,
-73.99539647, -73.96742325, -74.00741679, -73.78861455)
lat <- c(40.74381839, 40.77222538, 40.7645971, 40.75711012, 40.69372876, 40.77691675, 40.72765373,
40.7960495, 40.70906677, 40.6477187)
location <- data.frame(longitude=long, latitude=lat)
nyc <- nyc + geom_point(data=data, aes(x=longitude, y=latitude,color=rating))
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
nycmap <- get_map("New York City", zoom=10)
nyc <- ggmap(nycmap) + xlim (-74.1, -73.8) + ylim (40.7, 40.85)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
nycmap <- get_map("New York City", zoom=15)
nyc <- ggmap(nycmap) + xlim (-74.1, -73.8) + ylim (40.7, 40.85)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
nycmap <- get_map("New York City", zoom=20)
nyc <- ggmap(nycmap) + xlim (-74.1, -73.8) + ylim (40.7, 40.85)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
len(long)
length(long)
length(lat)
View(location)
nycmap <- get_map("New York City", zoom=20)
nyc <- ggmap(nycmap)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
nycmap <- get_map("New York City", zoom=5)
nyc <- ggmap(nycmap)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
nycmap <- get_map("New York City", zoom=8)
nyc <- ggmap(nycmap)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
nycmap <- get_map("New York City", zoom=9)
nyc <- ggmap(nycmap)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
nycmap <- get_map("New York City", zoom=10)
nyc <- ggmap(nycmap)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
nycmap <- get_map("New York City", zoom=10.5)
nycmap <- get_map("New York City", zoom=11)
nyc <- ggmap(nycmap)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
library(ggmap)
library(ggplot2)
long <- c(-73.96953876, -73.95145817, -73.99075265, -73.94264002, -73.87230566, -74.00194481,
-73.9814195, -73.7685344 , -73.98209553)
lat <- c(40.75966902, 40.79166805, 40.74533487, 40.70977994, 40.76815091, 40.7205635,
40.66584058, 40.65431133, 40.77430645)
location <- data.frame(longitude=long, latitude=lat)
nycmap <- get_map("New York City", zoom=11)
nyc <- ggmap(nycmap)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
library(ggmap)
library(ggplot2)
long <- c(-73.98227551, -73.94894565, -73.97328617, -73.98660181, -73.87124998,
-73.97492762, -73.95253233, -73.78222692, -73.99468622, -74.0047403)
lat <- c(40.72726685, 40.8091506, 40.75436116, 40.76263573, 40.76696224, 40.78705872,
40.77328378, 40.64736145, 40.74292497, 40.70852686)
location <- data.frame(longitude=long, latitude=lat)
nycmap <- get_map("New York City", zoom=11)
nyc <- ggmap(nycmap)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
nycmap <- get_map("New York City", zoom=10)
nyc <- ggmap(nycmap)
nyc <- nyc + geom_point(data=location, aes(x=longitude, y=latitude))
nyc
setwd("~/Documents/15fall/Thesis/Columbia")
profile <- read.csv("data/allProfile.csv")
summary(profile$statuses_count)
hist(profile$followers_count)
hist(profile$friends_count)
